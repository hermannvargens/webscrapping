{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9cc575c",
   "metadata": {},
   "source": [
    "PARTE 1: Abrir a página do pregão que contém as informações básicas e os links para cada item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3792b30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf6b6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\n",
    "# URL da página do pregão\n",
    "url = \"https://contratos.sistema.gov.br/transparencia/compras?lei=LEI14133&lei_text=LEI14133&unidade_origem_id=9964&unidade_origem_id_text=160224+-+PARQUE+REGIONAL+DE+MANUTENCAO%2F5&modalidade_id=76&modalidade_id_text=05+-+Preg%C3%A3o\"\n",
    "\n",
    "# Obtendo a data atual\n",
    "data_atual = datetime.date.today()\n",
    "\n",
    "def iniciar():\n",
    "    if os.path.exists('url_pregoes.csv'):\n",
    "        try:\n",
    "            url_pregoes_antiga = carregar_lista_pregoes()\n",
    "                        \n",
    "            url_pregoes_nova = baixar_nova_lista_pregoes(url)\n",
    "            \n",
    "            df_itens = verificar_se_ha_novos_pregoes(url_pregoes_antiga, url_pregoes_nova)\n",
    "            \n",
    "            salvar_dados()\n",
    "            \n",
    "            return 'Dados Salvos com sucesso!'\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Erro ao carregar lista de pregões: {e}.')\n",
    "    else:\n",
    "        print('Não há lista de pregões salvas nesse computador.')\n",
    "        url_pregoes_antiga = []\n",
    "        \n",
    "        #Baixando lista de pregões\n",
    "        print('Obtendo a url dos pregões...')\n",
    "        url_pregoes_nova = baixar_nova_lista_pregoes(url)\n",
    "        \n",
    "        print('Obtendo a url dos itens...')      \n",
    "        url_itens = obter_url_dos_itens(url_pregoes_nova)\n",
    "        \n",
    "        print('Obtendo os dados dos itens...')\n",
    "        df_itens = atualizar_todos_os_dados(url_itens)\n",
    "    \n",
    "        salvar_dados()\n",
    "        \n",
    "        return 'Dados Salvos com sucesso!'\n",
    "\n",
    "def carregar_lista_pregoes():\n",
    "    nome_arquivo = 'url_pregoes.csv'\n",
    "    url_pregoes_antiga = []\n",
    "    \n",
    "    with open(nome_arquivo, mode='r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)  # Pular o cabeçalho\n",
    "        url_pregoes_antiga = [row[0] for row in reader]\n",
    "    \n",
    "    print('Lista de Pregões salvas no PC encontrada!')\n",
    "        \n",
    "    return url_pregoes_antiga\n",
    "\n",
    "def baixar_nova_lista_pregoes(url):\n",
    "    \n",
    "    print('Baixando nova lista de url dos pregões ...')\n",
    "    # Configuração do WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Executar em modo headless para não abrir uma janela do navegador\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Selecionar a opção \"Todos\"\n",
    "    select = Select(driver.find_element(By.NAME, 'crudTable_length'))\n",
    "    select.select_by_value('-1')\n",
    "    time.sleep(5)  # Esperar alguns segundos para garantir que a página carregue completamente\n",
    "\n",
    "    # Obter e parsear o HTML da página\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    specific_url_part = 'https://contratos.sistema.gov.br/transparencia/compras/'\n",
    "    url_pregoes_nova = [a_tag['href'].replace('show', 'itens') for a_tag in soup.find_all('a', href=True) if specific_url_part in a_tag['href']]\n",
    "    \n",
    "\n",
    "\n",
    "    # Gravar a lista em um arquivo CSV\n",
    "    nome_arquivo = 'url_pregoes.csv'\n",
    "    with open(nome_arquivo, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([f'Atualizado em {data_atual}'])  # Escrever a data da atualização\n",
    "        writer.writerows([[url] for url in url_pregoes_nova])\n",
    "        print('Nova Lista de url dos pregões salva com sucesso!')\n",
    "    \n",
    "    return url_pregoes_nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a7922e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_se_ha_novos_pregoes(url_pregoes_antiga, url_pregoes_nova):\n",
    "    \n",
    "    novos_pregoes = list(set(url_pregoes_nova) - set(url_pregoes_antiga))\n",
    "    \n",
    "    if novos_pregoes == {}:\n",
    "        \n",
    "        print('Não há novos pregões. Iniciando atualização dos saldos...')\n",
    "\n",
    "        #chamar função de atualizar o saldo dos pregões existentes\n",
    "    \n",
    "        df_itens = atualizar_saldo()\n",
    "        \n",
    "        return df_itens\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print('Há novos pregões não cadastrados! \\n Obtendo os dados dos novos pregões...') \n",
    "        \n",
    "        url_itens_novos = obter_url_dos_itens(novos_pregoes)\n",
    "\n",
    "        df_itens_novos = atualizar_todos_os_dados(url_itens_novos)\n",
    "        \n",
    "        print('Dados dos novos pregões atualizados! Atualizando o saldo dos pregões anteriores...')\n",
    "        \n",
    "        df_itens_antigos = atualizar_saldo()\n",
    "        \n",
    "        print('Saldo dos pregões anteriores atualizado. Concatenando a tabela dos itens antigos com os novos...')\n",
    "        \n",
    "        df_itens = pd.concat([df_itens_novos, df_itens_antigos])\n",
    "        \n",
    "        return df_itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83f8432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_dados():\n",
    "    \n",
    "    #Salvando o df em arquivo csv\n",
    "    nome_arquivo = \"df_itens.csv\"\n",
    "\n",
    "    # Exportando o DataFrame para um arquivo CSV com delimitador ';' e codificação UTF-8\n",
    "    df_itens.to_csv(nome_arquivo, sep=';', encoding='utf-8', index=False)\n",
    "\n",
    "    print('Arquivo com a tabela dos itens salva com sucesso!')\n",
    "    \n",
    "    #Gravar a lista em um arquivo CSV\n",
    "    \n",
    "    df_itens['Link dos Item'].to_csv('url_itens.csv', sep=';', encoding='utf-8', index=False)\n",
    "    \n",
    "    print('Lista com a url dos itens salva com sucesso!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7bf2cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_url_dos_itens(novos_pregoes):\n",
    "    \n",
    "    url_itens = []\n",
    "\n",
    "    for url in novos_pregoes:       \n",
    "\n",
    "    # Configuração do WebDriver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')  # Executar em modo headless para não abrir uma janela do navegador\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Encontrar o elemento select pelo ID\n",
    "        select_element = driver.find_element(By.NAME, 'crudTable_length')\n",
    "\n",
    "        # Criar um objeto Select\n",
    "        select = Select(select_element)\n",
    "\n",
    "        # Selecionar a opção \"Todos\"\n",
    "        select.select_by_value('-1')\n",
    "\n",
    "        # Esperar alguns segundos para garantir que a página carregue completamente\n",
    "        time.sleep(5)\n",
    "\n",
    "        # Obter o HTML da página\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Fechar o WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "        # Parsear o HTML com Beautiful Soup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Encontrar os links para cada item\n",
    "        specific_url_part = 'https://contratos.sistema.gov.br/transparencia/compras/'\n",
    "\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            if specific_url_part in href:\n",
    "                url_itens.append(href)    \n",
    "\n",
    "    url_itens = [url for url in url_itens if 'show' in url]\n",
    "    \n",
    "        \n",
    "    return url_itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1b1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualizar_todos_os_dados(url_itens):\n",
    "\n",
    "    linhas = []\n",
    "\n",
    "    for url in url_itens:\n",
    "        \n",
    "        # Enviar uma solicitação GET para obter o conteúdo da página\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Verificar se a solicitação foi bem-sucedida\n",
    "\n",
    "        if response.status_code == 200:              \n",
    "\n",
    "            # Parsear o HTML da página com Beautiful Soup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            #Pregão\n",
    "            pregao = soup.find_all('div', class_='header-title')[1].text.strip().replace('Itens da compra: 160224 - ','')\n",
    "\n",
    "            # Encontrar todas as tabelas na página\n",
    "            tables = soup.find_all('table')\n",
    "            #Define a nova linha\n",
    "            linha = [\n",
    "                pregao, #Pregão\n",
    "                tables[0].find_all('span')[0].text.strip(), #Item\n",
    "                tables[0].find_all('span')[2].text.strip(), #Descrição\n",
    "                tables[0].find_all('span')[3].text.strip(), #Descrição detalhada\n",
    "                #tables[0].find_all('span')[4].text.strip(), #Qtd. Total\n",
    "                tables[0].find_all('span')[5].text.strip(), #Vig. Início ARP\n",
    "                tables[0].find_all('span')[6].text.strip(), #Vig. Fim ARP\n",
    "                tables[1].find_all('td')[0].text.strip(), #Unidade\n",
    "                #tables[1].find_all('td')[1].text.strip(), #Tipo UASG\n",
    "                tables[1].find_all('td')[2].text.strip(), #Qtd. Autorizada\n",
    "\n",
    "                tables[2].find_all('td')[0].text.strip(), #Fornecedor\n",
    "                #tables[2].find_all('td')[1].text.strip(), #Qtd. Homologada\n",
    "                tables[2].find_all('td')[2].text.strip(), #Val. Unitário\n",
    "                #tables[2].find_all('td')[3].text.strip(), #Val. Negociado      \n",
    "                tables[1].find_all('td')[3].text.strip(), #Qtd. Saldo\n",
    "                url, #link de cada item no Comprasgov\n",
    "                ]\n",
    "            #Append a linha\n",
    "            linhas.append(linha)\n",
    "            \n",
    "        #Para obter os cabeçalhos\n",
    "\n",
    "        headers = [\n",
    "            \"Número da Compra\",\n",
    "            tables[0].find_all('strong')[0].text.strip(), #Item\n",
    "            tables[0].find_all('strong')[2].text.strip(), #Descrição\n",
    "            tables[0].find_all('strong')[3].text.strip(), #Descrição detalhada\n",
    "            #tables[0].find_all('strong')[4].text.strip(), #Qtd. Total\n",
    "            tables[0].find_all('strong')[5].text.strip(), #Vig. Início ARP\n",
    "            tables[0].find_all('strong')[6].text.strip(), #Vig. Fim ARP\n",
    "            tables[1].find_all('th')[0].text.strip(), #Unidade\n",
    "            #tables[1].find_all('th')[1].text.strip(), #Tipo UASG\n",
    "            tables[1].find_all('th')[2].text.strip(), #Qtd. Autorizada\n",
    "\n",
    "            tables[2].find_all('th')[0].text.strip(), #Fornecedor\n",
    "            #tables[2].find_all('th')[1].text.strip(), #Qtd. Homologada\n",
    "            tables[2].find_all('th')[2].text.strip(), #Val. Unitário\n",
    "            #tables[2].find_all('th')[3].text.strip(), #Val. Negociado\n",
    "            tables[1].find_all('th')[3].text.strip(), #Qtd. Saldo\n",
    "            'Link do Item'\n",
    "        ]\n",
    "    \n",
    "        headers[1] = \"Número do Item\"\n",
    "        headers[2] = \"Descrição\"\n",
    "        headers[3] = \"Descrição Detalhada\"\n",
    "        headers[4] = \"Início da Vigência\"\n",
    "        headers[5] = \"Fim da Vigência\"\n",
    "        \n",
    "        #transformando os dados em um dataframe\n",
    "        \n",
    "        df_itens = pd.DataFrame(linhas, columns = headers)\n",
    "\n",
    "        #Corrigindo os valores que são numéricos\n",
    "\n",
    "        df_itens['Qtd. Autorizada'] = df_itens['Qtd. Autorizada'].str.replace('.','')\n",
    "        df_itens['Qtd. Autorizada'] = df_itens['Qtd. Autorizada'].str.replace(',','.')\n",
    "\n",
    "        df_itens['Val. Unitário'] = df_itens['Val. Unitário'].str.replace('.','')\n",
    "        df_itens['Val. Unitário'] = df_itens['Val. Unitário'].str.replace(',','.')\n",
    "\n",
    "        df_itens['Qtd. Saldo'] = df_itens['Qtd. Saldo'].str.replace('.','')\n",
    "        df_itens['Qtd. Saldo'] = df_itens['Qtd. Saldo'].str.replace(',','.')\n",
    "\n",
    "        #Inserindo novas colunas\n",
    "\n",
    "        tipo_compra = []\n",
    "        numero_compra_pregao = []\n",
    "        ano_compra_pregao = []\n",
    "\n",
    "\n",
    "        for i in range(len(df_itens['Número da Compra'])):\n",
    "            tipo = df_itens['Número da Compra'][i].split(' ')[0]\n",
    "            numero_pregao = df_itens['Número da Compra'][i].split(' ')[2].split('/')[0]\n",
    "            ano_pregao = df_itens['Número da Compra'][i].split(' ')[2].split('/')[1]\n",
    "\n",
    "            tipo_compra.append(tipo)\n",
    "            numero_compra_pregao.append(numero_pregao)\n",
    "            ano_compra_pregao.append(ano_pregao)\n",
    "\n",
    "        df_itens['Tipo de Compra'] = tipo_compra\n",
    "        df_itens['Número do Pregão'] = numero_compra_pregao\n",
    "        df_itens['Ano do Pregão'] = ano_compra_pregao\n",
    "\n",
    "        return df_itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda49ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualizar_saldo():\n",
    "    \n",
    "    #Carregando df_itens\n",
    "    \n",
    "    print('Carregando DataFrame com os dados dos itens...')\n",
    "    \n",
    "    df_itens = pd.read_csv('df_itens.csv')\n",
    "    #Para obter as informações de cada item\n",
    "    \n",
    "    for i in range(len(df_itens['Link do item'])):\n",
    "        \n",
    "        url = df_itens['Link do item'][i]\n",
    "        \n",
    "        # Enviar uma solicitação GET para obter o conteúdo da página\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Verificar se a solicitação foi bem-sucedida\n",
    "    \n",
    "        if response.status_code == 200:  \n",
    "\n",
    "            # Parsear o HTML da página com Beautiful Soup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Encontrar todas as tabelas na página\n",
    "            tables = soup.find_all('table')\n",
    "\n",
    "            #Define a nova linha\n",
    "            saldo = tables[1].find_all('td')[3].text.strip() #Qtd. Saldo\n",
    "\n",
    "            #Gravar saldo\n",
    "            df_itens['Qtd. Saldo'][i] = saldo\n",
    "\n",
    "            print('Saldo do item ' + {str(i)} + 'atualizado com sucesso!')\n",
    "\n",
    "    df_itens['Qtd. Saldo'] = df_itens['Qtd. Saldo'].str.replace('.','')\n",
    "    df_itens['Qtd. Saldo'] = df_itens['Qtd. Saldo'].str.replace(',','.')\n",
    "    \n",
    "    return df_itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fbca7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não há lista de pregões salvas nesse computador.\n",
      "Obtendo a url dos pregões...\n",
      "Baixando nova lista de url dos pregões ...\n",
      "Nova Lista de url dos pregões salva com sucesso!\n",
      "Obtendo a url dos itens...\n",
      "Obtendo os dados dos itens...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_itens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43miniciar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 40\u001b[0m, in \u001b[0;36miniciar\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObtendo os dados dos itens...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     38\u001b[0m df_itens \u001b[38;5;241m=\u001b[39m atualizar_todos_os_dados(url_itens)\n\u001b[1;32m---> 40\u001b[0m \u001b[43msalvar_dados\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDados Salvos com sucesso!\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m, in \u001b[0;36msalvar_dados\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m nome_arquivo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdf_itens.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Exportando o DataFrame para um arquivo CSV com delimitador ';' e codificação UTF-8\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mdf_itens\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(nome_arquivo, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArquivo com a tabela dos itens salva com sucesso!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Gravar a lista em um arquivo CSV\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_itens' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    iniciar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e64b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
